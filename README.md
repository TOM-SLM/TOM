<div align="center">
  <h1>TOM: Tiny-Omni-Model</h1>
</div>

# 📌 概述

**TOM（Tiny-Omni-Model）** 是一个专为**学习多模态大模型原理**而打造的**轻量级大语言模型框架。**

它秉持 **“小而全”** 的设计理念，在普通个人电脑或多种硬件环境中均可部署运行，同时保留主流大语言模型的核心能力和训练方法。

无论你是初学者想系统理解**LLM**架构与训练流程，还是研究者需要一个可控、可扩展的原型系统，**TOM**都能通过结构清晰的模块加上详尽的中文注释，让你在本地即可完成**构建 → 训练 → 微调 → 推理**的完整学习与实践闭环。

# 🎯 核心目标
1. 理论学习 - 深入理解多模态大模型的架构设计与原理
2. 技术掌握 - 学会从零开始训练、微调和扩展模型
3. 能力全覆盖 - 在轻量化框架中体验主流大模型的核心功能
4. 低门槛部署 - 支持**CPU**及多种**GPU**架构，无需昂贵硬件

# 🔑 功能范围

TOM 支持从基础到高级的多种大模型能力：
- **预训练 (Pretrain)** — 从零构建语言模型
- **监督微调 (SFT)** — 基于任务特定数据优化模型
- **参数高效微调 (LoRA)** — 在低资源设备快速更新模型参数
- **混合专家模型 (MoE)** — 高效利用多专家网络
- **视觉理解 (Vision)** — 多模态任务（文字+图片）处理
- **函数调用 (Function Call)** — 让模型调用外部工具或 API 完成任务
- **智能体 (Agent)** — 多步任务执行与组合工具调用
- **任务规划 (Planning)** — LLM 主导的决策与执行计划生成
- **代码生成 (Coding)** — 自动编写、调试与运行代码

# 📚 快速开始

## 从0开始自己训练

### 1. 训练分词器（Tokenizer）

TOM 分词器仅保留约 8k 词汇，相比市面动辄百万级的词汇表更轻量，但依然涵盖了绝大部分中文和常用表情符号。在主要面向中文的高质量数据集支持下，这一设计显著降低了存储与计算开销，同时保证模型在中文场景下的表现。

```bash
python scripts/train_tokenizer.py
```

> 训练分词器是一个CPU密集型任务，通常比较耗时，可以直接使用代码中的分词器。

### 2. 预训练（Pretrain）

```bash
python scripts/train_pretrain.py
```

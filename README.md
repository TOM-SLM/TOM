<div align="center">
  <h1>TOM: Tiny-Omni-Model</h1>
</div>

# 📌 概述

**TOM（Tiny-Omni-Model）** 是一个专为**学习与掌握多模态大模型原理**而打造的**开源、轻量级、多模态语言模型框架。**

它秉持 **“小而全”** 的设计理念，在普通个人电脑或多种硬件环境中均可部署运行，同时保留主流大语言模型的核心能力和训练方法。

无论你是初学者想系统理解**LLM**架构与训练流程，还是研究者需要一个可控、可扩展的原型系统，**TOM**都能通过结构清晰的模块加上详尽的中文注释，让你在本地即可完成**构建 → 训练 → 微调 → 推理**的完整学习与实践闭环。

# 🎯 核心目标
1. 理论学习
    - 深入理解多模态大模型的架构设计与原理
2. 技术掌握
    - 学会从零开始训练、微调和扩展模型
3. 能力全覆盖
    - 在轻量化框架中体验主流大模型的核心功能
4. 低门槛部署
    - 支持**CPU**及多种**GPU**架构，无需昂贵硬件

# 🔑 功能范围

TOM 支持从基础到高级的多种大模型能力：
- **预训练 (Pretrain)** — 从零构建语言模型
- **监督微调 (SFT)** — 基于任务特定数据优化模型
- **参数高效微调 (LoRA)** — 在低资源设备快速更新模型参数
- **混合专家模型 (MoE)** — 高效利用多专家网络
- **视觉理解 (Vision)** — 多模态任务（文字+图片）处理
- **函数调用 (Function Call)** — 让模型调用外部工具或 API 完成任务
- **智能体 (Agent)** — 多步任务执行与组合工具调用
- **任务规划 (Planning)** — LLM 主导的决策与执行计划生成
- **代码生成 (Coding)** — 自动编写、调试与运行代码

# 📚 快速开始

## 从0开始自己训练

### 1. 训练分词器（Tokenizer）

TOM 分词器仅保留约 8k 词汇，相比市面动辄百万级的词汇表更轻量，但依然涵盖了绝大部分中文和常用表情符号。在主要面向中文的高质量数据集支持下，这一设计显著降低了存储与计算开销，同时保证模型在中文场景下的表现。

```bash
python scripts/train_tokenizer.py
```

> 训练分词器是一个CPU密集型任务，通常比较耗时，可以直接使用代码中的分词器。

### 2. 预训练（Pretrain）

```bash
python scripts/train_pretrain.py
```


# 🖥️ 运行环境与硬件支持

- **部署类型**
    - CPU（x86 / ARM 架构）
    - 单机单卡
    - 单机多卡
    - 多机多卡，支持分布式训练
- **GPU 支持**
    - NVIDIA CUDA
    - AMD ROCm
    - Apple Metal（M 系列芯片）

在上述环境下，**TOM**都能完成训练、推理、微调等任务。

# 👥 适用人群

- **AI学习者**：希望零起点掌握大模型原理与实践
- **研究人员**：进行算法验证、架构探索、原型实验
- **教育工作者**：在课程或工作坊中演示大模型训练与能力扩展
- **个人开发者**：在家用电脑上快速试验 LLM 新功能

# 📜 开源协议

本项目基于 Apache License 2.0 发布：
- 可自由使用、修改和分发，包括商用
- 必须在分发时保留原版权与许可证声明
- 可在衍生项目中使用其他许可证